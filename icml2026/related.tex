\section{Related work}\label{sec:related-work-pomdp}
There exist reinforcement learning algorithms that directly train decision tree policies optimizing the cumulative rewards in a given MDP.
These approaches can be divided into methods based on \emph{parametric} and \emph{non-parametric} trees.

Parametric decision trees fix the structure in advance and only learn decision thresholds, enabling differentiable optimization with policy gradients~\cite{pg_sutton}. Several works~\cite{silva,vos2024optimizinginterpretabledecisiontree,sympol} train such trees with PPO, but the fixed structure limits adaptively balancing interpretability and performance, often requiring pruning or stabilization tricks~\cite{sympol}.

Non-parametric trees instead grow structure during training, as in supervised learning~\cite{breiman1984classification,oct}, but extending this to optimize cumulative MDP reward remains largely unexplored~\cite{milani-survey}. To our knowledge, only~\citet{topin2021iterative} study this setting via iterative bounding MDPs (IBMDPs), where certain policies correspond to downstream decision tree policies and can be learned with standard RL.

A few specialized approaches also exist, e.g., constructive methods for mazes~\cite{theory1} or planning-based shallow trees in known MDPs~\cite{dt-opt-mdp}.