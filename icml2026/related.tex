\section{Related work}\label{sec:related-work-pomdp}
There exist reinforcement learning algorithms that directly train decision tree policies optimizing the cumulative rewards in a given MDP.
These approaches can be divided into methods based on \emph{parametric} and \emph{non-parametric} trees.

Parametric decision trees fix the tree structure \emph{a priori}—including depth, node arrangement, and selected state features—and only learn the decision thresholds.
This formulation enables differentiability and allows direct optimization of the RL objective using policy gradient methods~\cite{pg_sutton}.
Several works~\cite{silva,vos2024optimizinginterpretabledecisiontree,sympol} employ PPO to train such differentiable trees.
While these methods can achieve strong performance, they require the tree structure to be specified in advance, making it difficult to adaptively trade off interpretability and performance.
An overly complex structure may require post-hoc pruning, whereas an insufficiently expressive structure may fail to represent good policies.
Moreover,~\citet{sympol} reports that additional stabilization techniques, such as adaptive batch sizes, are often necessary for direct reinforcement learning to match indirect imitation methods~\cite{viper} performances.

Non-parametric decision trees, by contrast, are the standard model in supervised learning, where algorithms efficiently construct trees that balance predictive performance and interpretability~\cite{breiman1984classification,oct}.
This trade-off is optimized by using a regularized training objective.
However, training non-parametric trees to trade-off MDP cumulative rewards and interpretability is largely unexplored~\cite{milani-survey}.
To the best of our knowledge, the only work that studies this setting:~\citet{topin2021iterative}.
Topin et al. introduce \emph{iterative bounding MDPs} (IBMDPs), which augment the downstream MDP with additional state features, actions, rewards, and transitions.
They show that certain policies in the IBMDP correspond to decision tree policies for the downstream MDP. Hence, standard RL algorithms can be used to learn such policies in IBMDPs.
% This framework provides a principled approach to learning non-parametric decision tree policies in sequential decision making.

Finally, a few specialized methods exist for restricted problem classes.
For maze-like MDPs,~\citet{theory1} proves the existence of optimal decision tree policies and provides a constructive algorithm.
In settings where the MDP model is fully known,~\citet{dt-opt-mdp} use planning to compute shallow parametric decision tree policies.
Next, we recall useful technical material.