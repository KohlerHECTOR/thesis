\section{Bridging the gap with the partially observable MDPs literature}\label{sec:poibmdp}
\subsection{An adequate formalism}
To better understand reinforcement learning of decision tree policies for MDPs, we explictly re-write the problem of optimizing a deterministic policy depending on current observations in an IBMDP as the problem of optimizing a deterministic policy depending only on current observations--also known as a deterministic \emph{memoryless} policy--in a partially observable Markov decision process (POMDP~\cite{POMDP}).
By doing so, we can leverage results from the POMDP literature that is richer than interpretable reinforcement learning literature.

\begin{definition}[Partially observable Markov decision process]\label{def:pomdp}
A partially observable Markov decision process is a tuple $\langle X, A, O, R, T, T_0, \Omega\rangle$. $X$ is the hidden state space.
$A$ is a finite set of actions.
$O$ is a set of observations.
$T: X \times A \rightarrow \Delta(X)$ is the transition function, where $T(\boldsymbol{x}_t, a, \boldsymbol{x}_{t+1}) = P(\boldsymbol{x}_t|\boldsymbol{x}_{t+1}, a)$ is the probability of transitioning to state $\boldsymbol{x}_{t}$ when taking action $a$ in state $\boldsymbol{x}$.
$T_0$: is the initial distribution over states. 
$\Omega: X \rightarrow \Delta(O)$ is the observation function, where $\Omega(\boldsymbol{o}, a, \boldsymbol{x}) = P(\boldsymbol{o}|\boldsymbol{x}, a)$ is the probability of observing $\boldsymbol{o}$ in state $\boldsymbol{x}$.
$R: X \times A \rightarrow \mathbb{R}$ is the reward function, where $R(\boldsymbol{x}, a)$ is the immediate reward for taking action $a$ in state $\boldsymbol{x}$.
Note that $\langle X, A, R, T, T_0 \rangle$ defines an MDP (definition~\ref{def:mdp}).
\end{definition}

Next, we can define partially observable iterative bounding Markov decision processes (POIBMDPs). 
They are IBMDPs (definition~\ref{def:ibmdp}) for which we explicitly define an observation space and an observation function.

\begin{definition}[Partially observable iterative bounding Markov decision process]\label{def:poibmdp} a partially observable iterative bounding Markov decision process $\mathcal{M}_{POIB}$ is a tuple:
    \begin{align*}
        \langle \overbrace{S\times O}^{\text{States}}, \overbrace{A\cup A_{info}}^{\text{Action space}},\overbrace{O}^{\text{Observations}} ,\overbrace{(R, \zeta)}^{\text{Rewards}}, \overbrace{(T_{info}, T, T_0)}^{\text{Transitions}}, \Omega \rangle
    \end{align*}
    , where $\langle S\times O, A\cup A_{info}, (R, \zeta),( T, T_0, T_{info})\rangle$ is an IBMDP (definition~\ref{def:ibmdp}).
    The transition function $\Omega$ maps concatenation of state features and observations--IBMDP states--to observations, $\Omega:S\times O \rightarrow O$, with $P(\boldsymbol{o}|(\boldsymbol{s}, \boldsymbol{o}))=1$ 
\end{definition}
POIBMDPs are particular instances of POMDPs where the observation function simply applies a mask over some features of the hidden state.
This setting has other names in the literature.
For example, POIBMDPs are mixed observability MDPs \cite{momdp} with downstream MDP state features as the \textit{hidden variables} and feature bounds as \textit{visible} variables.
POIBMDPs can also be seen as non-stationary MDPs (N-MDPs)~\cite{learning-pomdp} in which there is one different transition function per downstream MDP state: these are called hidden-mode MDPs~\cite{hmmdp}.
Following~\cite{learning-pomdp} we can write the value of a deterministic memoryless policy $\pi:O\rightarrow A\cup A_{info}$ in observation $\boldsymbol{o}$.

\begin{definition}[Value of an observation]\label{def:vpo} In a POIBMDP (definition~\ref{def:poibmdp}), the expected cumulative discounted reward of a deterministic memoryless policy $\pi:O\rightarrow A\cup A_{info}$ starting from observation $o$ is $V^{\pi}(\boldsymbol{o})$:
$V^{\pi}(\boldsymbol{o}) &= \underset{(s,\boldsymbol{o}')\in S\times O}{\sum}P^{\pi}((\boldsymbol{s}, \boldsymbol{o}')|\boldsymbol{o})V^{\pi}((\boldsymbol{s}, \boldsymbol{o}'))$.
With $P^{\pi}((\boldsymbol{s}, \boldsymbol{o}')|\boldsymbol{o})$ the asymptotic occupancy distribution (see section 4 from~\citet{learning-pomdp} for the full definition) of the hidden POIBMDP state $(\boldsymbol{s},\boldsymbol{o}')$ given the partial observation $o$ and $V^{\pi}((s, o'))$ the classical state-value function (definition~\ref{def:vs}).
We abuse notation and denote both values of observations and values of states by $V$ since the function input is not ambiguous.
\end{definition}

\subsection{Reinforcement learning in POMDP}\label{sec:asym}

In general, the policy that maximizes the RL objective (definition~\ref{def:mdp-obj}) in a POMDP (definition~\ref{def:pomdp}) maps ``belief states'' or observation histories to actions~\cite{chap2}.
Hence, those policies do not correspond to decicion trees since we require that policies depend only on the current observation (algorithm~\ref{alg:extract-tree}).
If we did not have this constraint, we could apply any standard RL algorithm to solve POIBMDPs by seeking policies depending on belief states or observations histories because those are sufficient to optimally control any POMDP~\cite{chap2,lambrechts2025informed}.

In particular, the problem of finding the optimal deterministic memoryless policies for POMDPs is NP-HARD, even with full knowledge of transitions and rewards(section3.2 from~\citet{littman1}).
It means that it is impractical to enumerate all possible policies and take the best one. 
For even moderate-sized POMDPs, a brute-force approach would take a very long time since there are $|A|^{|O|}$ deterministic memoryless policies.
Hence it is interesting to study reinforcement learning for finding the best deterministic memoryless policy since it would not enumerate the whole solution space.

In~\citet{learning-pomdp}, the authors show that the optimal memoryless policy can be stochastic. Hence, policy gradient algorithms~\cite{pg_sutton}--that return stochastic policies--are to avoid since we seek the best \textit{deterministic} policy. 
Furthermore, the optimal deterministic memoryless policy might not maximize all the values of all observations simultaneously~\cite{learning-pomdp} which makes it difficult to use TD-learning algorithms like Q-learning~\cite{watkins1992q}.
Indeed, doing a TD-learning update of one observation's value (definition~\ref{def:vpo}) can change the value of \textit{all} other observations in an uncontrollable manner because of the dependence in $P^{\pi}((s, \boldsymbol{o}')|\boldsymbol{o})$ (definition~\ref{def:vpo}).

Despite theoretical hardness, simple RL methods that treat observations as states (e.g., Q-learning or Sarsa~\cite{sutton}) often work well in POMDPs~\cite{sarsa-pomdp}. More recently, asymmetric RL~\cite{baisero-dqn,baisero-ppo} has shown promise by training both a hidden-state model (available only during training) and an observation- or history-based model for deployment.
In appendix~\ref{sec:hp-pomdp}, we present tabular asymmetric RL methods such as asymmetric Q-learning, which learns an observable Q-function $Q:O\times A\to\mathbb{R}$ using a hidden-state target $U:X\times A\to\mathbb{R}$. This matches the modified DQN approach of~\citet{topin2021iterative}, and we provide a reproducibility study in appendix~\ref{sec:reprod}.

While asymmetric RL was long supported mainly by empirical results,~\citet{justif-asym} recently proved it can theoretically improve history-dependent or memoryless policies in POMDPs. However, these methods are currently impractical, as they require costly Monte Carlo estimates of occupancy distributions. We leave their practical integration to future work.