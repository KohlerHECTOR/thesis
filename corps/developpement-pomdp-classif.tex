\chapter{When transitions are uniform POIBMDPs are fully observable}
In this section we show that decision tree induction for classification problems can be formulated as solving POIBMDPs.
Indeed, a classification problem can be formulated as an MDP where actions are class labels and states are training data.
The reward at every step is 1 if the correct label was predicted and 0 otherwise.
In this MDP, the transitions are independent of the actions: the next state is given by uniformly sampling a new training datum. 
This implies that the resulting POIBMDP is actually \textit{fully} observable and that traditional RL algorithms should perform well.

After showing the equivalence between decision tree induction and solutions to POIBMDPs when the underlying base MDP encodes a classification task, we call them Classification-POIBMDPs, we highlight key limitations of the RL approach for decision tree induction.
There are few other RL approaches for decision tree induction (cite). Here we focus on RL for Classification-POIBMDPs
We defer the formal definition of classification tasks in the supervised learning setting in the next chapter and rather directly define an MDP that encodes such tasks. 
\begin{definition}[Classification Markov Decision Process]
    Given a set of $N$ examples denoted $\mathcal{E} = {\{(x_i, y_i)\}}_{i=1}^N$ where each datum $x_i$ is described by a set of $p$ features and $y_i \in \mathbb{Z}^m$ is the label associated with $x_i$, a Classification Markov Decision Process is an MDP (cite) $\langle S, A, R, T, T_0 \rangle$ where:
    \begin{itemize}
        \item the state space is $S={\{x_i\}}_{i=1}^N$, the set of data features
        \item the action space is $A=\mathbb{Z}^m$, the set of unique labels
        \item the reward function is $R:S\times A \rightarrow \{0, 1\}$ with $R(s=x_i, a) = 1_{a=y_i}$
        \item the transition function is $T:S\times A \rightarrow \Delta S$ with $T(s, a, s') = \frac{1}{N} \quad \forall s, a, s'$
        \item the initial distribution is $T_0(s_0 = s) = \frac{1}{N}$
    \end{itemize}
\end{definition}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        decision/.style={circle, draw, thick, fill=blue!20, text width=2.5em, text centered, minimum height=2.5em, font=\small},
        leaf/.style={rectangle, draw, thick, fill=green!20, text width=2em, text centered, rounded corners, minimum height=2em, font=\small},
        edge_label/.style={font=\footnotesize, midway}
    ]
        % Tree 4: if x <= 0.5 move right else move left
        \node[decision] (tree4_root) at (8,2) {$x \leq 1$};
        \node[rectangle, draw, thick, fill=green!40, text width=2em, text centered, rounded corners, minimum height=2em, font=\small] (tree4_right) at (7,0) {};
        \node[rectangle, draw, thick, fill=red!40, text width=2em, text centered, rounded corners, minimum height=2em, font=\small] (tree4_left) at (9,0);
        \draw[->] (tree4_root) -- (tree4_right) node[edge_label, above left] {True};
        \draw[->] (tree4_root) -- (tree4_left) node[edge_label, above right] {False};
        \tikzstyle{grid}=[draw, thick, fill=gray!10]
        
        % Draw grid
        \draw[fill=green!40] (0, 0) rectangle (1,2);
        \draw[fill=red!40] (1, 0) rectangle (2,2);

        \draw[grid] (0,0) grid (2,2);
        
        % Add axes
        \draw[thick, ->] (0,0) -- (2.5,0) node[right] {$x$};
        \draw[thick, ->] (0,0) -- (0,2.5) node[above] {$y$};
        
        % Add tick marks and labels
        \foreach \x in {0,1,2} {
            \draw[thick] (\x,0) -- (\x,-0.1) node[below] {$\x$};
        }
        \foreach \y in {0,1,2} {
            \draw[thick] (0,\y) -- (-0.1,\y) node[left] {$\y$};
        }

        \node at (0.5,0.5) {$s_0$};
        \node at (1.5,0.5) {$s_g$};
        \node at (1.5,1.5) {$s_2$};
        \node at (0.5,1.5) {$s_1$};

    \end{tikzpicture}
    \caption{Classification MDP optimal actions. An optimal policy that takes any of the red actions in the bottom right state might not be imitated by the best depth-1 tree in terms of performance interpretability trade-offs.
    Center, an optimal depth-1 decision tree policy. On the right, a sub-optimal depth-1 decision tree policy.}
    \end{figure}

It is easy to see that policies that maximize the expected dsicounted cumulative reward objective (cite) are classifiers that maximize the prediction accuracy $\sum_{i=1}^N 1_{\pi(x_i)=y_i} = \sum_{i=1}^N R(x_i, \pi(x_i))$.
Learning a classifier with a reward signal can also be done with contextual bandits (cite).

To learn a decision tree classifier, we use the POIBMDP formalism and show in the particular case of the base MDP being a Classification MDP, the POIBMDP is in fact an MDP with stochastic transitions. 

\begin{definition}[Classification POIBMDP]
    Given a classification MDP (cite) $\langle {\{x_i\}}_{i=1}^N, \mathbb{Z}^m, R, T, T_0 \rangle$, and an associated POIBMDP (cite) $\langle S, O, A, A_{info}, R, \zeta, T_{info}, T, T_0\rangle$, a classification POIBMDP is an MDP:
    \begin{align*}
        \langle \overbrace{O}^{\text{State space}}, \underbrace{\mathbb{Z}^m, A_{info}}_{\text{Action space}}, \overbrace{R, \zeta}^{\text{Reward function}}, \underbrace{\mathcal{P}, \mathcal{P}_0}_{\text{Transition kernels}} \rangle
    \end{align*}
    \begin{itemize}
        \item $O$ is the set of possible observations in $[L_1, U_1] \times \dots \times [L_d, U_d] \times [L_1, U_1] \ times \dots \times [L_d, U_d] $ where $L_j$ is the minimum value of feature $j$ over all data $x_i$ and $U_j$ the maximum
        \item $\mathbb{Z}^m \cup A_{info}$ is action space: actions can be label assignments in $\mathbb{Z}^m$ or bounds refinement in $A_{info}$
        \item The reward for assigning label $a\in \mathbb{Z}^m$ when observing some observation $o=(L'_1, U'_1, \dots, L'_d, U'_d)$ is the proportion of training data satistifying the bounds and having label $a$: $R(o, a) = \frac{|\{x_i: L'_j \leq x_{ij} \leq U'_j \forall i,j \} \cap \{x_i: y_i = a \forall i \}|}{|\{x_i: L'_j \leq x_{ij} \leq U'_j \forall i,j \}|}$. 
        The reward for taking an information gathering action that refines bounds is $\zeta$
        \item The transition kernel is $\mathcal{P}:O \times (\mathbb{Z}^m \cup A_{info}) \rightarrow \Delta O$ where:
        \begin{itemize}
            \item For $a \in \mathbb{Z}^m$: $\mathcal{P}(o, a, (L_1, U_1, \dots, L_d, U_d)) = 1$ (reset to full bounds)
            \item For $a = (k, v) \in A_{info}$: from $o=(L'_1, U'_1, \dots, L'_d, U'_d)$, the MDP will transit to $o_{left} = (L'_1, U'_1, \dots, L_k, v, dots, L'_d, U'_d)$ (resp. $o_{right} = (L'_1, U'_1, \dots, U'_k, v, dots, L'_d, U'_d)$) with probability $\frac{|\{x_i: L'_j \leq x_{ij} \leq U'_j \forall j \land x_{ik} \leq v\}|}{|\{x_i: L'_j \leq x_{ij} \leq U'_j \forall j\}|}$ (resp. $\frac{|\{x_i: L'_j \leq x_{ij} \leq U'_j \forall j \land x_{ik} > v\}|}{|\{x_i: L'_j \leq x_{ij} \leq U'_j \forall j\}|}$)
        \end{itemize}
    \end{itemize}
\end{definition}

Now we can learn a decision tree \textit{classifier} for a supervised learning . 
Given a set of examples and lables, we can formulate the associated classification MDP (cite). 
Then we can provide some information gathering actions and a reward $\zeta$ for those to define a classification POIBMDP (cite).
Finally, we can learn a deterministic policy $\pi : O \rightarrow A\cup A_{info}$ to obtain a decision tree classifier. 
Since we have shown that classification POIBMDPs are MDPs, we can investigate how well RL baselines perform to solve those problems. 
We expect that, compared to standard POIBMDP studied in the previous chapter where some information was hidden to the agent, RL baselines will consistently learn the optimal decision tree classifiers.

\section{Results}

\subsection{How well can RL baselines learn in classification POIBMDPs?}
Similarly to the previous chapter, we are interested first in a very simple classification POIBMDP that corresponds to building a tree for the supervised problem:

\begin{align*}
    \mathcal{X} &= \{(0.5, 0.5), (0.5, 1.5), (1.5, 1.5), (1.5, 0.5)\}\\
    y &= \{0, 0, 1, 1\} 
\end{align*}

We illustrate the associated classification MDP in Figure (cite). 
We construct classification POIBMDPs with varied values of $\zeta$ and IGAs $x\leq 1$ and $y\leq 1$.
Similarly to the previous chapter, we will analyze learning curves and final tree distributions of RL agents.
Since classification POIBMDPs are MDP, we do not need to analyze asymmetric and JSJ (cite) baselines.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/images_part1/learning_curves_classif.pdf}
    \caption{}\label{fig:rl-classif-poibmdp}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/images_part1/tree_distributions_classif.pdf}
    \caption{}\label{fig:tree-distrib-classif-poibmdp}
\end{figure}

Fortunately this time, compared to general POIBMDPs, RL can be used to retrieve optimal policies in classification POIBMDPs equivalent to decision tree classifiers.
We observe on Figure (cite) that both Q-learning and Sarsa are able to retrieve the optimal detph-1 decision tree classifier from Figure (cite).
Next, we use Deep RL to scale to real datasets for which we don't have much a priori knowledge such that which bvalue of $\zeta$ to choose to obain a particular tree, or which IGAs to use to get good trees.

\subsection{Can we use Deep RL to learn trees in real-world classification POIBMDPs ?}

We study the DQN (cite) algorithm to learn decision trees for the wine dataset.
In particular, we will look at four DQN variants.
First, default DQN on a classification POIBMDP where the IGAs are, for each feature $i$, :
This variant is equivalent to running CUSTARD from (cite).
Then a variant where we consider less IGAs.
Then a variant where we impose a maximum tree depth during training by penalizing long uninterupted sequences of IGAs.
The latter was recommended to us by the authors of (cite).
The last DQN variant combines both a small number of IGAs and a maximum depth penalty.

\begin{figure}
    \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/images_part1/training_curves.pdf}
    \caption{}\label{fig:tree-distrib-classif-poibmdp}
    \end{subfigure}
    
    \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/images_part1/training_curves_lessp.pdf}
    \caption{}\label{fig:tree-distrib-classif-poibmdp}
    \end{subfigure}
    
    \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/images_part1/training_curves_md3.pdf}
    \caption{}\label{fig:tree-distrib-classif-poibmdp}
    \end{subfigure}
   
    \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/images_part1/training_curves_lessp_md3.pdf}
    \caption{}\label{fig:tree-distrib-classif-poibmdp}
    \end{subfigure}
\caption{}
\end{figure}

It is clear from the figures that DQN and perhaps classification POIBMDPs are not well suited for learning decision trees classifiers.
In the next part of this thesis; we present a new decision tree induction algorithm solving MDPs exactly and choosing information gathering actions adaptively.



