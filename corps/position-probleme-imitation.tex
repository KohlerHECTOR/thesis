\chapter{Imitation and Evaluation}
\section{Intro}
There exist applications of reinforcement learning like medicine where policies need to be ``interpretable'' by humans. User studies have shown that some policy classes might be more interpretable than others. However, it is costly to conduct human studies of policy interpretability. Furthermore, there is no clear definition of policy interpretabiliy, i.e., no clear metrics for interpretability and thus claims depend on the chosen definition. We tackle the problem of empirically evaluating policies interpretability without humans. Despite this lack of clear definition, researchers agree on the notions of ``\textit{simulatability}'': policy interpretability should relate to how humans understand policy actions given states. To advance research in interpretable reinforcement learning, we contribute a new methodology to evaluate policy interpretability. We distillate expert neural networks policies into small programs that we use as baselines. We then show that using our methodology to evaluate the baselines interpretability leads to similar conclusions as user studies. Most importantly, we show that there is no policy class that better trades off interpretability and performance across tasks. 

There is increasing research in developing reinforcement learning algorithms that return ``interpretable" policies such as trees, programs, first-order logic, or linear maps \cite{viper,PIRL,empirical-evidence,nudge,milani-survey,glanois-survey,kohler2024interpretableeditableprogrammatictree}. Indeed, interpretability has been useful for different applications: policy verification \cite{viper}, mis-alignment detection \cite{scobots,sympol} and features importance analysis \cite{fi-rl,fi-rl2,fi-rl3}.
%Those concerns are at the core of our work. 

User studies have established the common beliefs that decision trees are more ``interpretable" than linear maps, oblique trees (trees where nodes are tests of linear combinations of features), and multi-layer perceptrons (MLPs)~\cite{study-0,study-1,study-2,study-3}. Furthermore, for a fixed class of models, humans give different values of interpretability to models with different numbers of parameters~\cite{study-4}. However, survey works argue that every belief about interpretability needs to be verified with user studies and that interpretability evaluations are grounded to a specific set of users, to a specific application, and to a specific definition of interpretability \cite{rigorous,mythos}. For example, \cite{mythos} claims that depending on the notion of \textit{simulatability} studied, MLPs can be more interpretable than trees, since deep trees can be harder for a human to read than compact MLPs. Hence, even with access to users it would be difficult to research interpretability. More realistically, since the cost of user studies is high (time, variety of subjects required, ethics, etc.), designing proxies for interpretability in machine learning has become an important open problem in both supervised~\cite{rigorous} and reinforcement learning~\cite{glanois-survey}.

In this work, we propose a methodology to evaluate the interpretability of reinforcement learning without human evaluators, by measuring inference times and memory consumptions of policies as programs. We show that those measures constitute adequate proxies for the notions of ``\textit{simulatability}'' described in~\cite{mythos}, which relates the interpretability of policy to humans ability to understand the inference of actions given states. In addition to the contributions summarized next, we open source some of the interpretable baselines to be be used for future interpretability research and teaching\footnote{\url{https://anonymous.4open.science/r/interpretable-rl-zoo-4DCC/README.md}}.
