\chapter*{Preliminary Concepts}

\section{What is Sequential Decision Making?}
In this manuscript we are interested in sequential decision making processes. Sequential decision making processes are found in all aspects of life. In medicine, doctors have to decide when to use chimiotherapy next based on the patient's current health in order to heal (cite). In agriculture, agronomists have to decide when to fertilize next based on the current soil and wheather conditions in order to maximize plant growth (cite). In automotive, the auto-pilot system has to decide how to steer the wheel next based on lidar sensors in order to maintain a safe trajectory (cite). In video games, a bot decides what attack to throw next based on the player's and its own state in order to provide the best entertainment (cite).
Those sequential decision making processes exhibits key similarities: an agent takes actions based on some current information to achieve some goal. 
Algorithms presented in this thesis use the formalism of Marko decision processes that we present next to model sequential decision making.
\subsection{Markov decision processes/problems}
Markov decision processes (MDPs) were first introduced in the 1950s by Richard Bellman (cite). The original idea behind MDPs was to fromalize some dynamical systems which states obey the Markov property--the futur only depends on the the present and is independent of the past--and hence can be solved efficiently with dynamic programming (cite). Following Martin L. Puterman's book on MDPs (cite), we formally define an MDP as  
\subsection{Exact solutions}

\subsection{Reinforcement learning}

\section{What is Interpretable Sequential Decision Making?}

\subsection{Why do we care about interpretability?}


\subsection{What are existing approaches for interpretable sequential decision making?}


\subsection{What are decision tree policies?}


\subsection{Why are decision tree policies harder to learn than decision tree classifiers?}

\section{Outline of the Thesis}