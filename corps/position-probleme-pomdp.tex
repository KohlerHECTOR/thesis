\chapter{A Decision Tree Policy for an MDP is a Policy for some Partially Observable MDP}
\epigraphhead[30]{\selectlanguage{english}\epigraph{I have not failed. I've
    just found 10.000 ways that won't work.}{Thomas A. Edison}}

    In this part of the manuscript we first make the case for direct interpretable methods (cite)(poupart) but highlight some challenges.
We start by considering a very basic Markov decision process (def) for which we would like decision tree (def) policies. 
 
\section{POIBMDPs is a direct method}
In 2021, Topin et. al. introduced Iterative Bounding Markov decision process (IBMPD) (cite). It is, to the best of our knowledge, the only work in which decision tree policies are \textit{grown} using the RL object (def).
We insist here on \textit{growing} to refer to trees whose nodes and leaves structure is not knows \textit{a priori} as opposed to parametric trees--trees which structure is fixed-- and can be optimized with the policy gradient theorem (cite).
In supervised learning a similar distinction exists too (cite). 

In this seminal work Topin et. al., formulate learning a decision tree policy for an MDP as solving an augmented MDP where some actions are adding decision nodes to a tree structure or taking base actions. (figure)
More interestingly, IBMDP can generalize to any sort of tree-like policies where nodes need only to be binary functions of features.  

\section{Litterature and results on POMDPs}
Without mentionning explicitely Topin et.al. propose to learning in IBMDPs by solving a Partially Observable MDP (cite). POMDPs are notorously more difficult to solve than MDPs (cite).
In particular, not all policies for $\mathcal{M}_IB$ are decision tree policies for $\mathcal{M}$. Only policies from partial observation to actions are trees. 
Hence learning a decision tree policy for an MDP can be done by learning a \textit{deterministic} and \textit{reactive} policy for a POMDP. 
In general, this problem cannot be solved exactly with dynamic programming because of intrinsic POMDP limitations described in the work of Michael Littman (cite).

We will show that three of those limitations arise in direct interpretable RL:
\begin{enumerate}
    \item The optimal policy in a POMDP can be stochastic
    \item The optimal policy in a POMDP does not necessarily maximizes all state values simultaneously
    \item Q-Learning like algorithms do not necessirily converge to the optimal Q-values.
\end{enumerate}

In addition to those limitaions that are inherited from POMDPs, IBMDPs bring their own intrinsic challenges:
\begin{enumerate}
    \item The action space might need to be state dependent
    \item It is hard to align RL agents for IBMDPs
\end{enumerate}

\section{IBMDP and Didactic example}