\chapter{A Framework for the Reinforcement Learning of Decision Tree Policies}
\section{Learning Decision Tree policies}
Deicision tree policies offer transparency over neural network policies (cite). 
One can attribute an importance measure to each feature of the state for the deicison of a tree policy which is harder to do for neural networks (cite).

Recently, algorithms have been developed to return decision tree policies for an MDP.
Those algorithms, like any interpretable machine learning method, are either direct or indrect (cite).
We propose an additional distinction amongst the direct methods: algorithms learning parametric trees and algorithms learning non-parametric trees.

Parametric trees are not ``grown'' from the root by iteratively adding internal or leaf nodes depending on the interpretability-performance trade-off to optimize, but are rather ``optimized'': the depth, internal nodes arrengement, and state-features to consider in each nodes are fixed \textit{a priori} and only the tested thresholds of each nodes are optimized similarly to how the weights of a neural network are optimized.
As the reader might have guessed, those parametric trees are advantageous in that they can be learned with gradient descent and in the context of decision tree policies, with the policy gradient (cite).
The downside of those approaches is that a user cannot know \textit{a priori}  what a ``good'' tree policy structure should be for a particular MDP: either the specified structure is too deep and pruning will be required after training or the tree structure is not expressive enough to encode a good policy. 
Similar approaches exist in supervised learning exist where a parametric tree is fitted with gradient descent (cite) GRADTREE and the work of the authors in KDD. However their benefit over non-parametric trees have not been shown.
When parametric trees are learned for MDPs (cite); extra stabilizing tricks are required during training such as adaptive batch sizes.

Non-parametric trees are the standard model in supervised learning (cite) and can naturally trade-off between interpretability and performances. However, specialized approaches are required since growing a tree from the root in an RL fashion is not possible.
In the next section we present, to the best of our knowledge, the only direct approach to learn non-parametric decision tree policies for MDPs; Iterative Bounding MDPs (cite). 

Other more specialized approaches deal with tree policies either for specific MDPs like maze (cite) or for very small depth (cite) or when the MDP model is known (cite)
\section{Iterative Bounding Markov Decision Processes}
In 2021, Topin et. al. introduced Iterative Bouding Markov Decision Processes (IBMDPs) with the promise of sticking the challenges of non-parametric decision tree policy learning in the problem formulation rather than in the learning algorithms.
Given a base MDP for which one wants to learn a decision tree policy, IBMDPs are an augmented version of this base MDP with more state features, more actions, additinal reward signal, and additional transition kernel.
Then author showed that certain IBMDP policies, that can be learned with RL, are equivalent to decision tree policies for the base MDP. 
\subsection{Formalism}
The key thing to know about IBMDPs is that they are, as their name suggests, MDPs. Hence they inherit all their properties such as existence of a deterministic optimal Markovian policy.
The states in an IBMDP are concatenations of base MDP states and some observations. Those observations are some information about the base states that are refined--``iteratively bounded''-- at each step and represent a a subspace of the base MDP state space.
Actions available in an IBMDP are the actions of the base MDP, that change the state of the latter, and \textit{information gathering} actions that change the observation part of the IBMDP state.
Now, taking base actions in an IBMDP is rewarded like in the base MDP, this ensures that base objective, e.g. balancing the pole or treating cancer, is still encoded in the IBMDP. When taking such \textit{information gathering} actions; the reward is an arbitrary value supposed to trade-off between performance and interpretability.
 
Before showing how to get decision trees from IBMDP policies, we give a formal definition of the latter following Topin et. al. (cite).

\begin{definition}[Iterative Bounding Markov decision process]
Given a \textit{factored} (cite) MDP $\mathcal{M}$ (cite), that is, a tuple $\langle S, A, R, T, T_0 \rangle$ with $S\subsetneq \mathbb{R}^n$, an Iterative Bouding MDP $\mathcal{M_ib}$ is a tuple:
\begin{align*}
    \langle \overbrace{S, O}^{\text{State space}}, \underbrace{A, A_{info}}_{\text{Action space}}, \overbrace{R, \zeta}^{\text{Reward function}}, \underbrace{T_{info}, T, T_0}_{\text{Transition kernels}}\rangle
\end{align*}

\begin{itemize}
\item $S$ the base MDP state space should be of the form $S = [L_1, U_1]\times \dots \times [L_n, U_n]$ with $\infty < L_i \leq U_i < \infty \forall 1\leq i\leq n$.
\item $O$ are the observations in an IBMDP. They are partial information about the values of base MDP states: $O\subsetneq S^2 =  [L_1, U_1]\times \dots \times [L_n, U_n] \times [L_1, U_1]\times \dots \times [L_n, U_n]$. So the complete IBMDP state space is $(S, O) = S \times O$ the concatenations of states and observations.
\item $A$ are the actions of the base MDP.
\item $A_{info}$ are added \textit{information gathering} actions (AIGs) of the form $\langle i, v \rangle$ where $i$ is a state feature index $1 \leq i \leq n$ and $v$ is a real number. So the complete action space of an IBMDP is the set of base actions and \textit{information gathering} actions $A \cup A_{info}$.
\item $R: S\times A \rightarrow \mathbb{R}$ is the base MDP reward function that maps base states and actions to a real-valued reward signal.
\item $\zeta$ is a reward signal for taking an \textit{information gathering} action. So the IBMDP reward function is to get a reward from the base MDP if the action is a base MDP action or to get $\zeta$ if the action is a \textit{information gathering} action.
\item $T_{info}: S\times O \times( A_{info} \cup A )\rightarrow \Delta (S\times O)$ is the transition kernel of IBMDPs. 
Given the current observation $o_{t} = (L'_1, U'_1, \dots, L'_n, U'_n)\in O$ and the current state is $s_t=(s_1, s_2, \dots, s_n)$ if an AIG $\langle i, v \rangle$ is taken, only the bounds in the observation change:
\begin{align*}
    o_{t+1} &= \begin{cases}
        (L'_1, U'_1, \dots , L'_i, \min\{v, U'_i\}, \dots , L'_n, U'_n) \text{ if } s_i \leq v\\
        (L'_1, U'_1, \dots , \max\{v, L'_i\}, U'_i, \dots , L'_n, U'_n) \text{ if } s_i > v
    \end{cases}
\end{align*}
If a base action $a\in A$ is taken, $o_{t+1}$ is reset to the default state bounds $(L_1, U_1,\dots, L_n, U_n)$ and the base state changes according to the base MDP transitition kernel: $s_{t+1}\sim T(s, a)$.
At initialization, the base part of the IBMDP states is drawn from $T_0$ and the observation is set always set to $(L_1, U_1,\dots, L_n, U_n)$.
The overall IBMDP transitions are given by either $T_{info}$, which i fully deterministic, if an AIG is played, and by the base MDP's transition kernel otherwise.
\end{itemize}
\end{definition}
Now remains the question why policies of IBMDPs are decision tree policies of their base MDP? 

\subsection{From Policies to Trees}
\begin{algorithm}[t]
    \KwData{IBMDP policy $\pi$ and observation $o=(L'_1, U'_1, \dots, L'_n, U'_n)$}
    \KwResult{Decision tree policy extracted from $\pi$}
    
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwFunction{SubtreeFromPolicy}{Subtree\_From\_Policy}
    
    \Fn{\SubtreeFromPolicy{$o, \pi$}}{
        $a \leftarrow \pi(o)$ \\
        \If{$a \in A_{info}$}{
            \Return Leaf\_Node(action: $a$) \Comment{// Leaf if base action}
        }
        \Else{
            $\langle i, v\rangle \leftarrow a$ \Comment{// Splitting action is feature and value} \\
            $o_L \leftarrow o; \quad o_R \leftarrow o$ \\
                         $o_L \leftarrow (L'_1, U'_1, \dots, L'_i, v, \dots, L'_n, U'_n); \quad o_R \leftarrow (L'_1, U'_1, \dots, v, U'_i, \dots, L'_n, U'_n)$ \\
            $child_L \leftarrow$ Subtree\_From\_Policy$(o_L, \pi)$ \\
            $child_R \leftarrow$ Subtree\_From\_Policy$(o_R, \pi)$ \\
            \Return Internal\_Node(feature: $i$, value: $v$, children: $(child_L, child_R)$)
        }
    }
    
    \Return Subtree\_From\_Policy$(obs, \pi)$
    \caption{Extract a Decision Tree Policy from an IBMDP policy $\pi$, beginning traversal from $obs$.}\label{alg_extract_tree}
\end{algorithm}

One can notice that \textit{information gathering} actions resemble the Boolean functions that make up internal decision tree nodes (cite). 
Indeed, an agent evolving in an IBMDP essentially builds a tree by taking sequences of AIGs (internal nodes) and then a base action (leaf node) and repeats this process over time.
However not all IBMDP policies are decision tree policies. In particular, only deterministic policies depending solely on the observation part of the IBMDP states $\pi: O \rightarrow A$ are decision tree policies $\pi_{\mathcal{T}} S \rightarrow A$for the base MDP (cite).
Algorithm (cite) from (cite) extracts a decision tree policy for the base MDP from a determenistic partailly observable IBMDP policy.

\subsection{Didactic example}

\subsection{Partially Observable IBMDPs}

So now we know that to find a decision tree policy for a given MPD $\mathcal{M}$ satisfying def (cite); one has to find a partially observable deterministic policy for an IBMDP $\mathcal{M}_{IB}$.
Such problems are classical instances of Partially Observable MDPs (POMDPs) (cite). This connection with POMDP was not done by the authors of IBMDPs. 

\begin{definition}[Partially Observable Markov Decision Processes]
A Partially Observable Markov Decision Process (POMDP) is a tuple $\langle X, A, O, T, T_0, \Omega, R\rangle$ where:
\begin{itemize}
    \item $X$ is the state space (like in the base definition of MDPs).
    \item $A$ is a finite set of actions (like in the base definition of MDPs).
    \item $O$ is a set of observations.
    \item $T: X \times A \rightarrow \Delta X$ is the transition kernal, where $T(s, a, x') = P(x'|x, a)$ is the probability of transitioning to state $x'$ when taking action $a$ in state $x$
    \item $T_0$: is the intial distribution over states. 
    \item $\Omega: X \rightarrow \Delta O$ is the observation kernel, where $\Omega(x', a, o) = P(o|x', a)$ is the probability of observing $o$ in state $x$
    \item $R: X \times A \rightarrow \mathbb{R}$ is the reward function, where $R(x, a)$ is the immediate reward for taking action $a$ in state $x$
\end{itemize}
Note that $\langle X, A, R, T, T_0 \rangle$ defines an MDP (cite).
\end{definition}

And so we can simply extend the definition of Iterative Bounding MDPs (cite) with an observation kernel to get Partially Observable IBMDPs:
\begin{definition}[Partially Observable Iterative Bounding Markov Decision Processes] a Partially Observable Iterative Bounding Markov Decision Process (IBMDP) is a an IBMDP extended with an observation kernel 
    \begin{align*}
        \langle \overbrace{S, \underbrace{O}_{\text{Observations}}}^{\text{fully observable states X}}, \underbrace{A, A_{info}}_{A}, \overbrace{R, \zeta}^{R}, \underbrace{T_{info}, T, T_0}_{T, T_0}, \Omega \rangle
    \end{align*}
\end{definition}

The sole specifity of Partially observable IBMDPs compared to the general definition of POMDPs, is that $\Omega(o|(s, o'))$, the probability of observing $o$ in $(s,o')$, is $1_{o=o'}$.

Following (cite) we can write the definition of the value of a deterministic partially observable policy $\pi:O\rightarrow A$ in observation $o$:
Similarly we can define value functions that depend only on the partial observations $O$ following (cite):

\begin{definition}[Partial observable value function] The expected cumulative discounted reward a deterministic partially observable policy $\pi:O\rightarrow A$ starting from observation $o$ is $V^{\pi}(o)$:
    \begin{align*}
        V^{\pi}(o) &= \underset{(s,o')\in S\times O}{\sum}P^{\pi}((s, o')|o)V^{\pi}((s, o'))
    \end{align*}
with $P^{\pi}((s, o')|o)$ the asymptotic occupancy distribution (see cite for definition) of the fully observable state $(s,o')$ given the partial observation $o$ and $V^{\pi}((s, o'))$ the classical state-value function defined in (cite).
\end{definition}


The asymptotic occupancy distribution is the probability of a policy $\pi$ to be in $(s,o')$ while observing $o$ and having taken actions given by $\pi$.  

The problem that we solve is to find the deterministic partially observable policy that maximizes the excpeted value in the initial observation:
\begin{align}
    \pi^{\star} &= \underset{\pi}{\operatorname{argmax}}J(\pi) = \underset{\pi}{\operatorname{argmax}}V^{\pi}(o_0)
\end{align}
With $\pi:O\rightarrow A$. There is no expectation over possible initial observation in the above objective function as in there is only one initial observation in a (PO)IBMDP: $o_0=(L_1, U_1, \dots, L_n, U_n)$.




This particular instance of POMDPs that we try to solve are called CITE ALL NAMES OF THIS PARTICULAR POMDP (c.f. Olivier Buffet).

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    box/.style={rectangle, draw, minimum width=4cm, minimum height=3cm, align=center},
    inner box/.style={box, fill=blue!10},
    middle box/.style={box, fill=green!10},
    outer box/.style={box, fill=red!10}
]
    % Nested boxes (left side)
    \node[outer box, anchor=south west] (poibmdp) at (0,0) {
        \textbf{POIBMDP}\\
        $+\Omega$
    };
    
    \node[middle box, scale=0.8, anchor=south west] (ibmdp) at (0,0) {
        \textbf{IBMDP}\\
        $+O, T_{info}$
    };
    
    \node[inner box, scale=0.4, anchor=south west] (base) at (0,0) {
        \textbf{Base MDP}\\
        $\langle S, A, R, T, T_0 \rangle$
    };
    
    % Hyperparameters arrows
    \draw[dashed, thick, ->] (-1.5, 3) -- (-0.3, 1.5) node[midway, above, sloped] {$+\zeta, A_{info}$};
    \draw[dashed, thick, ->] (5.5, 3) -- (4.7, 1.5) node[midway, above, sloped] {$+\gamma$};
    
    % Objective function box (top right)
    \node[box, fill=yellow!10] (objective) at (5, 0) {
        \textbf{Objective}\\
        $\pi^{\star} = \underset{\pi}{\operatorname{argmax}} V^{\pi}(o_0)$
    };
    
    % Policy box (bottom right)
    \node[box, fill=orange!10] (policy) at (5, -4) {
        \textbf{Policy}\\
        $\pi^{\star}: O \rightarrow A$
    };
    
    % Decision tree box (bottom left)
    \node[box, fill=purple!10] (tree) at (0, -4) {
        \textbf{Decision Tree}\\
        \includegraphics[width=1.5cm]{images/images_intro/decision-tree-svgrepo-com.svg}
    };
    
    % Main flow arrows
    \draw[thick, ->] (4, 1.5) -- (5, 1.5) node[midway, above] {solve};
    \draw[thick, ->] (5, -1) -- (5, -3) node[midway, right] {solve with\\e.g. RL};
    \draw[thick, ->] (4, -4) -- (1, -4) node[midway, above] {extract tree\\\small{(alg 6)}};
    
    % Final arrow from tree back to base MDP
    \draw[thick, ->] (0.5, -2.5) -- (0.5, -0.5) node[midway, right] {Can deploy\\and interpret};
    
\end{tikzpicture}
\caption{Nested structure of decision processes: Base MDP (innermost), IBMDP (middle), and POIBMDP (outermost).}
\label{fig:nested_decision_processes}
\end{figure}

How to choose $\zeta$? How to specify the action space? Is the horizon infinite ? Could we specify a maximum depth ? 
