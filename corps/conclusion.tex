\chapter*{General conclusion}
In this manuscript we have tackled the hard problem of interpretable sequential decision making.
We have studied interpretability through the prism of model classes used as policies for Markov deicison processes or predictors for classification tasks.
We put great emphasis on decision tree models that are often considered more interpretable than neural networks, now widespread in machine learning applications.

In the first part of this manuscript, we studied algorithms that learn non-parametric decision tree policies for Markov decision processes (cf. chapter~\ref{sec:intro-pomdp}).
In particular, we studied the reinforcement learning algorithms from~\cite{topin2021iterative} that use the reward signal of some augmented Markov decision process to train decision tree policies that optimize some trade-off of interpretability and performance in a sequential decision task.
In chapter~\ref{sec:topin}, we performed a reproducibility study of those algorithms and concluded that imitation learning algorithms, despite not optimizing the performances on the downstream tasks, yielded better decision tree policies with similar number of internal nodes.
To better understand why directly optimizing an interpretability-performance trade-off using reinforcement learning yielded bad decision tree policies, we showed in chapter~\ref{sec:pomdp} that the latter could be re-written as an optimization problem in a partially observable Markov decision processes.
Through extenisve experimentations with standard and specialized reinforcement learning algorithms in a very controlled environment, we identified partial observability as the main failure mode of direct reinforcement learning for deicison tree policies.
To further corroborate this conclusion, we showed that for the same class of partially observable Markov decision problems where this time the observations contained all the information about the hidden state, those reinforcement learning algorithms converged to the optimal decision tree policies.
Surprisingly, this class of easy to solve partially observable Markov decision problems contains supervised learning problems.

The previous observation prompted the design of a new decision tree induction algorithm for supervised learning.
In the second part of this manuscript, we introduced dynamic programming decision trees (DPDT), a novel decision tree induction framework.
In chapter~\ref{sec:dt-mdp}, we showed how to construct a Markov decision process where states are training data and actions are candidate decision tree nodes.
In chapter~\ref{sec:exps-dt}, we showed that adaptively choosing actions based on some greedy heuristics and then solving the Markov decision process with dynamic programming yielded state-of-the-art decision tree classifiers.
Extensive benchmarking highlights that DPDT computes near-optimal decision trees in a fraction of the time required by existing optimal solvers.
Most importantly, we showed that DPDT can be competitive in many machine learning applications as its generalization capabilities when generating single or ensemble of trees are also state-of-the-art compared to greedy and optimal algorithms.
While this part of the manuscript may have seemed orthogonal to the other two as the primary focus of algorithms studied there is downstream task performances rather than interpertability, this justifies studying interpretability for more than just transparency and safety, e.g. for perofrmances.


