\chapter*{Conclusion générale}
In this manuscript we have tackled the hard problem of interpretable sequential decision making.
We have studied interpretability through the prism of model classes used as policies for Markov deicison processes for classifiers for classification tasks.
We put great emphasis on decision tre models that are often considered more interpretable to so extent than neural networks now widespred in machine learning applications.

In the first part of this manuscript, we studied algorithms that learn non-parametric decision tree policies for Markov decision processes (cf. chapter~\ref{sec:intro-pomdp}).
In particular, we studied the reinforcement learning algorithms from~\cite{topin2021iterative} that use the reward signal of some augmented Markov decision process to train decision tree policies that optimize some trade-off of interpretability and performance in a sequential decision task.
In chapter~\ref{sec:topin}, we performed a reproducibility study of those algorithms and concluded that imitation learning algorithms, despite not optimizing the performances on the downstream tasks, yielded better decision tree policies with similar number of internal nodes.
To better understand why the directly optimizing an interpretability-performance trade-off with reinforcement learning underperformed, we showed in chapter~\ref{sec:poibmdp} that the latter could be re-written as an optimization problem in a partially observable Markov decision processes.
Through extenisve experimentations with standard and specialized reinforcement learning algorithms in a very controlled environment, we identified partial observability as the main failure mode of direct reinforcement learning for deicison tree policies.
To further corroborate this conclusion, we showed that for the same class of partially observable Markov decision problems where this time the observations contained all the information about the hidden state, those reinforcement learning algorithms converged to the optimal decision tree policies.
Surprisingly, this class of easy to solve partially observable Markov decision problems contains supervised learning problems.


